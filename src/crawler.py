# crawler.py

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
from urllib.parse import urljoin

from config import headers, BASE_URL, LIST_URL, FIELD_MAP, TARGET_KEYWORDS

def scrape_detail_page(url: str) -> dict | None:
    """
    ê°œë³„ ê³µëª¨ì „/ëŒ€ì™¸í™œë™ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë§ˆê°ëœ í™œë™ì¼ ê²½ìš° Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        detail_data = {}
        today = datetime.today()

        # ê¸°ë³¸ ì •ë³´ í…Œì´ë¸” íŒŒì‹±
        rows = soup.select(".txt_area > table > tbody > tr")
        for row in rows:
            th = row.find('th')
            td = row.find('td')
            if not th or not td or "ì½˜ì½” SNS ê³µìœ " in th.get_text():
                continue

            key = th.get_text(strip=True)
            value = re.sub(r'\s+', ' ', td.get_text(separator=' ', strip=True)).strip()

            # ë§ˆê°ì¼ ì²´í¬ (ì ‘ìˆ˜ê¸°ê°„, ì‹¬ì‚¬ê¸°ê°„ ê¸°ì¤€)
            if key in ["ì ‘ìˆ˜ê¸°ê°„", "ì‹¬ì‚¬ê¸°ê°„"]:
                date_match = re.search(r'(\d{2,4}\.\d{2}\.\d{2})\s*~\s*(\d{2,4}\.\d{2}\.\d{2})', value)
                if date_match:
                    end_str = date_match.group(2)
                    if len(end_str.split('.')[0]) == 2:
                        end_str = f"{today.year}.{end_str}"
                    end_date = datetime.strptime(end_str, "%Y.%m.%d")
                    if end_date < today:
                        return None # ë§ˆê°ëœ í•­ëª©ì´ë¯€ë¡œ None ë°˜í™˜

            # í™ˆí˜ì´ì§€ ë§í¬ ì¶”ì¶œ
            a_tag = td.find('a')
            if a_tag and a_tag.has_attr('href'):
                link_href = a_tag['href']
                if not link_href.startswith("http"):
                    link_href = BASE_URL + link_href
                value += f"({link_href})"

            if key in FIELD_MAP:
                detail_data[FIELD_MAP[key]] = value

        # ì°¸ê°€ëŒ€ìƒ í•„í„°ë§
        if "target" in detail_data and not any(kw in detail_data["target"] for kw in TARGET_KEYWORDS):
            return None

        # ì´ë¯¸ì§€ URL íŒŒì‹±
        img_tag = soup.select_one(".img_area > div > img")
        if img_tag and img_tag.has_attr("src"):
            img_src = img_tag["src"]
            detail_data["image"] = img_src if img_src.startswith("http") else BASE_URL + img_src

        # ìƒì„¸ ë‚´ìš©(h2/p) íŒŒì‹±
        detail_section = soup.select_one(".view_detail_area")
        details_dict = {}
        if detail_section:
            for h2 in detail_section.select("h2"):
                key = h2.get_text(strip=True)
                p = h2.find_next_sibling("p")
                if p:
                    details_dict[key] = re.sub(r'\s+', ' ', p.get_text(separator=' ', strip=True)).strip()
        detail_data["details"] = details_dict

        return detail_data

    except requests.RequestException as e:
        print(f"  [ì˜¤ë¥˜] ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url}, {e}")
        return None
    except Exception as e:
        print(f"  [ì˜¤ë¥˜] ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {url}, {e}")
        return None


def crawl_category(int_gbn: str, bcode: str) -> list[dict]:
    """
    ì£¼ì–´ì§„ ì¹´í…Œê³ ë¦¬(bcode)ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    all_items = []
    page = 26
    max_due_per_page = 12 # í•œ í˜ì´ì§€ì— ë§ˆê°ëœ í•­ëª©ì´ 12ê°œ ì´ìƒì´ë©´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™

    while True:
        print(f" > í˜ì´ì§€ : {page}")
        list_url = LIST_URL.format(int_gbn=int_gbn, bcode=bcode, page=page)
        
        try:
            res = requests.get(list_url, headers=headers)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
        except requests.RequestException as e:
            print(f"  [ì˜¤ë¥˜] ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {list_url}, {e}")
            break

        items = soup.select(".list_style_2 > ul > li > div > a")
        if not items:
            print("  - ëª©ë¡ ì—†ìŒ, í•´ë‹¹ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        due_count_this_page = 0
        for item_link in items:
            href = item_link.get("href")
            title = item_link.select_one("span.txt").get_text(strip=True)
            detail_url = urljoin(list_url, href)

            scraped_data = scrape_detail_page(detail_url)
            time.sleep(0.2) # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°

            if scraped_data:
                scraped_data["title"] = title # ëª©ë¡ì˜ ì œëª©ì´ ë” ì •í™•í•˜ë¯€ë¡œ ë®ì–´ì“°ê¸°
                all_items.append(scraped_data)
            else:
                due_count_this_page += 1
        
        if due_count_this_page >= max_due_per_page:
            print(f"  ğŸ“Œ ë§ˆê° í•­ëª© {due_count_this_page}ê°œ í™•ì¸, ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            break
        
        page += 1

    return all_items