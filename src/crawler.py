import requests
from bs4 import BeautifulSoup
import time
import json
import re
from datetime import datetime
from pathlib import Path

field = {
    "ì œëª©": "title",
    "ì´ë¯¸ì§€": "image",
    "ì£¼ìµœ . ì£¼ê´€": "host",
    "ëŒ€í‘œë¶„ì•¼": "category",
    "ì°¸ê°€ëŒ€ìƒ": "target",
    "ì ‘ìˆ˜ê¸°ê°„": "application_period",
    "ì‹¬ì‚¬ê¸°ê°„": "evaluation_period",
    "í™œë™ê¸°ê°„": "activity_period",
    "ëŒ€íšŒì§€ì—­": "location",
    "í™œë™ì§€ì—­": "location",
    "í™œë™í˜œíƒ": "benefits",
    "ì‹œìƒë‚´ì—­": "prize",
    "í™ˆí˜ì´ì§€": "source",
    "ì ‘ìˆ˜ë°©ë²•": "application_method",
    "ì ‘ìˆ˜í•˜ê¸°": "apply",
    "ì°¸ê°€ë¹„ìš©": "entry_fee"
}

today = datetime.today()
headers = {'User-Agent': 'Mozilla/5.0'}

# Contests (ëŒ€íšŒ ë° ê³µëª¨ì „)
# bcode: ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ [ë¬¸í•™â€¢ë¬¸ì˜ˆ,Â ë„¤ì´ë°â€¢ìŠ¬ë¡œê±´, í•™ë¬¸â€¢ê³¼í•™â€¢IT,Â ìŠ¤í¬ì¸ , ë¯¸ìˆ â€¢ë””ìì¸â€¢ì›¹íˆ°,Â ìŒì•…â€¢ì½©ì¿ ë¥´â€¢ëŒ„ìŠ¤,Â ì‚¬ì§„â€¢ì˜ìƒâ€¢ì˜í™”ì œ,Â ì•„ì´ë””ì–´â€¢ê±´ì¶•â€¢ì°½ì—…,Â ìš”ë¦¬â€¢ë·°í‹°â€¢ë°°ìš°â€¢ì˜¤ë””ì…˜,Â The ë‹¤ì–‘í•œ ë¶„ì•¼]
contest_bcode = ['030110001','030210001','030310001','030810001','030610001','030910001','031210001','031410001','031610001','031810001']
contests = {}

for bcode in contest_bcode:
    print(f"\nì¹´í…Œê³ ë¦¬: {bcode}")
    contests[bcode] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±

    page = 1
    max_due_per_page = 12  # í˜ì´ì§€ ë‹¹ í•­ëª© ìˆ˜

    while True:
        print(f"í˜ì´ì§€: {page}")
        view_links = []
        due_count_this_page = 0
        list_url = f"https://www.contestkorea.com/sub/list.php?int_gbn=1&Txt_bcode={bcode}&page={page}"
        res = requests.get(list_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')

        contest_items = soup.select(".list_style_2 > ul > li > div > a")

        for a in contest_items:
            href = a.get("href")
            title_span = a.select_one("span.txt")
            title = title_span.get_text(strip=True) if title_span else ""
            full_url = "https://www.contestkorea.com/sub/" + href
            view_links.append({
                "title": title,
                "url": full_url
            })

        for link in view_links:
            detail_res = requests.get(link["url"], headers=headers)
            detail_soup = BeautifulSoup(detail_res.text, 'html.parser')
            details = detail_soup.select(".txt_area > table > tbody > tr")
            
            detail_data = {}
            detail_data[field["ì œëª©"]] = link["title"]

            # ì´ë¯¸ì§€ url
            img_tag = detail_soup.select_one(".img_area > div > img")
            if img_tag and img_tag.has_attr("src"):
                img_src = img_tag["src"]
                if not img_src.startswith("http"):
                    img_src = "https://www.contestkorea.com" + img_src
                detail_data[field["ì´ë¯¸ì§€"]] = img_src

            skip_due = False

            for row in details:
                th = row.find('th')
                td = row.find('td')
                if not th or not td:
                    continue

                key = th.get_text(strip=True)
                value = td.get_text(separator=' ', strip=True)
                value = re.sub(r'\s+', ' ', value).strip()

                if not key or key == "ì½˜ì½” SNS ê³µìœ ":
                    continue

                # ë§ˆê°ëœ í•­ëª© ì œì™¸
                if key in ["ì ‘ìˆ˜ê¸°ê°„", "ì‹¬ì‚¬ê¸°ê°„"]:
                    date_match = re.search(r'(\d{2,4}\.\d{2}\.\d{2})\s*~\s*(\d{2,4}\.\d{2}\.\d{2})', value)
                    if date_match:
                        end_str = date_match.group(2)
                        if end_str.count('.') == 2 and len(end_str.split('.')[0]) == 2:
                            end_str = f"{today.year}.{end_str}"
                        try:
                            end_date = datetime.strptime(end_str, "%Y.%m.%d")
                        except ValueError:
                            continue  # ë‚ ì§œ í˜•ì‹ ì´ìƒ ì‹œ ê±´ë„ˆëœ€

                        if end_date < today:
                            skip_due = True
                            due_count_this_page += 1
                            break  # ìƒì„¸ í•­ëª© ë£¨í”„ íƒˆì¶œ

                # ì¶œì²˜ url
                a_tag = td.find('a')
                if a_tag and a_tag.has_attr('href'):
                    link_href = a_tag['href']
                    if not link_href.startswith("http"):
                        link_href = "https://www.contestkorea.com" + link_href
                    value += f"({link_href})"

                # í•œê¸€ â†’ ì˜ë¬¸ ë§¤í•‘ í›„ ì €ì¥
                if key in field:
                    mapped_key = field[key]
                    detail_data[mapped_key] = value

            # view_detail_area ë‚´ë¶€ h2/p ìŒ í¬ë¡¤ë§ â†’ details dictì— ì €ì¥
            detail_section = detail_soup.select_one(".view_detail_area")
            details_dict = {}
            if detail_section:
                h2_tags = detail_section.select("h2")
                for h2 in h2_tags:
                    key = h2.get_text(strip=True)
                    next_p = h2.find_next_sibling("p")
                    if next_p:
                        value = next_p.get_text(separator=' ', strip=True)
                        value = re.sub(r'\s+', ' ', value).strip()
                        details_dict[key] = value

            detail_data["details"] = details_dict  # "details" í‚¤ë¡œ ì¶”ê°€

            if skip_due:
                continue  # ì´ í•­ëª©ë§Œ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ
        
            # ëŒ€ìƒ: ëŒ€í•™ìƒ, ëŒ€í•™ì›ìƒ, ì¼ë°˜ì¸, í•´ë‹¹ì ì¸ í•­ëª© í•„í„°ë§
            if field["ì°¸ê°€ëŒ€ìƒ"] in detail_data:
                if not any(kw in detail_data[field["ì°¸ê°€ëŒ€ìƒ"]] for kw in ["ëˆ„êµ¬ë‚˜", "ëŒ€í•™ìƒ", "ëŒ€í•™ì›ìƒ", "ì¼ë°˜ì¸", "í•´ë‹¹ì â–¶"]):
                    continue

            contests[bcode].append(detail_data)
            

        # í˜ì´ì§€ ë‚´ ë§ˆê° í•­ëª©ì´ 12ê°œ ì´ìƒì´ë©´ í¬ë¡¤ë§ ì¢…ë£Œ
        if due_count_this_page >= max_due_per_page:
            print(f"ğŸ“Œ {page} í˜ì´ì§€ì—ì„œ ë§ˆê° í•­ëª©ì´ {due_count_this_page}ê°œ ë°œê²¬ë˜ì–´ í¬ë¡¤ë§ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        page += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ
        time.sleep(0.5)

# External Activities (ëŒ€ì™¸í™œë™)
# bcode: ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ [ì„œí¬í„°ì¦ˆâ€¢ê¸°ìë‹¨,Â ì²´í—˜â€¢íƒë°©â€¢ë´‰ì‚¬â€¢ë™ì•„ë¦¬,Â ì„œí‰ë‹¨â€¢ì°¸ì—¬ë‹¨â€¢í‰ê°€ë‹¨â€¢ìë¬¸ë‹¨,Â ê¸°íšâ€¢í™ë³´â€¢ë§ˆì¼€íŒ…,Â êµìœ¡â€¢ê°•ì—°â€¢ë©˜í† ë§â€¢ì„¸ë¯¸ë‚˜, ì „ì‹œâ€¢ë°•ëŒâ€¢í–‰ì‚¬â€¢ì¶•ì œ,Â The ë‹¤ì–‘í•œ ëŒ€ì™¸í™œë™]
ea_bcode = ['040110001','040210001','040310001','040710001','040410001','040510001','040610001']
ea = {}

for bcode in ea_bcode:
    print(f"\nì¹´í…Œê³ ë¦¬: {bcode}")
    ea[bcode] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±

    page = 1
    max_due_per_page = 12  # í˜ì´ì§€ ë‹¹ ë§ˆê° í•­ëª© ê¸°ì¤€

    while True:
        print(f"í˜ì´ì§€: {page}")
        list_url = f"https://www.contestkorea.com/sub/list.php?int_gbn=2&Txt_bcode={bcode}&page={page}"
        res = requests.get(list_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')

        ea_items = soup.select(".list_style_2 > ul > li > div > a")

        view_links = []
        for a in ea_items:
            href = a.get("href")
            title_span = a.select_one("span.txt")
            title = title_span.get_text(strip=True) if title_span else ""
            full_url = "https://www.contestkorea.com/sub/" + href
            view_links.append({
                "title": title,
                "url": full_url
            })

        due_count_this_page = 0  # ì´ë²ˆ í˜ì´ì§€ ë§ˆê° í•­ëª© ìˆ˜

        for link in view_links:
            detail_res = requests.get(link["url"], headers=headers)
            detail_soup = BeautifulSoup(detail_res.text, 'html.parser')
            details = detail_soup.select(".txt_area > table > tbody > tr")

            detail_data = {}
            detail_data[field["ì œëª©"]] = link["title"]

            # ì´ë¯¸ì§€ url
            img_tag = detail_soup.select_one(".img_area > div > img")
            if img_tag and img_tag.has_attr("src"):
                img_src = img_tag["src"]
                if not img_src.startswith("http"):
                    img_src = "https://www.contestkorea.com" + img_src
                detail_data[field["ì´ë¯¸ì§€"]] = img_src

            skip_due = False

            for row in details:
                th = row.find('th')
                td = row.find('td')
                if not th or not td:
                    continue

                key = th.get_text(strip=True)
                value = td.get_text(separator=' ', strip=True)
                value = re.sub(r'\s+', ' ', value).strip()

                if not key or key == "ì½˜ì½” SNS ê³µìœ ":
                    continue

                # ì ‘ìˆ˜ ë§ˆê°ëœ í•­ëª© ì œì™¸
                if key in ["ì ‘ìˆ˜ê¸°ê°„"]:
                    date_match = re.search(r'(\d{2,4}\.\d{2}\.\d{2})\s*~\s*(\d{2,4}\.\d{2}\.\d{2})', value)
                    if date_match:
                        end_str = date_match.group(2)
                        if end_str.count('.') == 2 and len(end_str.split('.')[0]) == 2:
                            end_str = f"{today.year}.{end_str}"
                        try:
                            end_date = datetime.strptime(end_str, "%Y.%m.%d")
                        except ValueError:
                            continue

                        if end_date < today:
                            skip_due = True
                            due_count_this_page += 1
                            break # ìƒì„¸ í•­ëª© ë£¨í”„

                # ì¶œì²˜ url
                a_tag = td.find('a')
                if a_tag and a_tag.has_attr('href'):
                    link_href = a_tag['href']
                    if not link_href.startswith("http"):
                        link_href = "https://www.contestkorea.com" + link_href
                    value += f"({link_href})"
                # í•œê¸€ â†’ ì˜ë¬¸ ë§¤í•‘ í›„ ì €ì¥
                if key in field:
                    mapped_key = field[key]
                    detail_data[mapped_key] = value

            # view_detail_area ë‚´ë¶€ h2/p ìŒ í¬ë¡¤ë§ â†’ details dictì— ì €ì¥
            detail_section = detail_soup.select_one(".view_detail_area")
            details_dict = {}
            if detail_section:
                h2_tags = detail_section.select("h2")
                for h2 in h2_tags:
                    key = h2.get_text(strip=True)
                    next_p = h2.find_next_sibling("p")
                    if next_p:
                        value = next_p.get_text(separator=' ', strip=True)
                        value = re.sub(r'\s+', ' ', value).strip()
                        details_dict[key] = value

            detail_data["details"] = details_dict  # "details" í‚¤ë¡œ ì¶”ê°€
                        
            if skip_due:
                continue  # ì´ í•­ëª©ë§Œ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ
            
            # ëŒ€ìƒ: ëŒ€í•™ìƒ, ëŒ€í•™ì›ìƒ, ì¼ë°˜ì¸, í•´ë‹¹ì ì¸ í•­ëª© í•„í„°ë§
            if field["ì°¸ê°€ëŒ€ìƒ"] in detail_data:
                if not any(kw in detail_data[field["ì°¸ê°€ëŒ€ìƒ"]] for kw in ["ëˆ„êµ¬ë‚˜", "ëŒ€í•™ìƒ", "ëŒ€í•™ì›ìƒ", "ì¼ë°˜ì¸", "í•´ë‹¹ì â–¶"]):
                    continue

            ea[bcode].append(detail_data)

        # í˜ì´ì§€ ë‚´ ë§ˆê° í•­ëª©ì´ 12ê°œ ì´ìƒì´ë©´ í¬ë¡¤ë§ ì¢…ë£Œ
        if due_count_this_page >= max_due_per_page:
            print(f"ğŸ“Œ {page} í˜ì´ì§€ì—ì„œ ë§ˆê° í•­ëª©ì´ {due_count_this_page}ê°œ ë°œê²¬ë˜ì–´ í¬ë¡¤ë§ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        page += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ
        time.sleep(0.5)
        
output_dir = Path("./output")
output_dir.mkdir(exist_ok=True)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
filename = output_dir / f"contestkorea_{timestamp}.json"

all_data = {
"contests": contests,
"external_activities": ea,
"crawled_at": datetime.now().isoformat()
}

with open(filename, "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ, ê²°ê³¼ë¥¼ '{filename}' ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print(f"ì¢…ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
